# Дерево принятия решений. Оценка разбиений

**Дерево решений** — алгоритм классификации и регрессии. Основные составляющие дерева:

- *Вершины* содержат разделяющие правила или вопросы, которые мы задаём к нашему датасету (зачастую вопросы обращены к признакам).
- *Ребро* — возможный ответ на вопрос в родительской вершине.
- *Листья* содержат решения (класс объекта для задачи классификации или число для задачи регрессии).

Построение дерева:

1. Поместим весь наш набор данных $S$ в корень дерева.
2. На каждом шагу мы будем рекурсивно обрабатывать выборку $S$.
   1. Если у нас выборка содержит объекты только одного класса $c$, то создаём лист с классом $c$ и останавливаемся.
   2. Иначе, мы выбираем правило $b \in \mathcal{B}$, которое является наилучшим с точки зрения критерия $\Phi$, и разделяем выборку на $k$ частей: $S_1,\,\ldots,\,S_k$, где $k \geqslant 2$.
   3. Посмотрим на критерий остановки. Если он выполнился, то возвращаем наиболее популярный класс в текущей выборке $S$. В ином случае мы выдаём $k$ подвыборок $S_1,\,\ldots,\,S_k$ и далее каждого ребенка обкатываем рекурсивно.
3. После чего мы подрезаем итоговое дерево.

В качестве оценки разбиения (или: [выбор критерия ветвления](../lectures/L6-DecisionTree.md#выбор-критерия-ветвления)) можно выбрать:

- *Энтропия*. Мера неопределенности данных в узле. Чем меньше энтропия, тем чище узел.
  - P.s. Я понимаю это так, чем больше энтропия, тем больше мы хотим разделить наш блок данных.

- *Информационный прирост*. Отношение прироста информации к нечистоте родительского узла. Пусть $S_0$ - это посчитанная энтропия на всей ещё неразделенной выборке, тогда приростом будет служить:

  $$
    \text{IGain} = S_0 - \sum_{i}{\dfrac{N_i}{N}S_i},
  $$

  то есть мы взяли вероятность встретить каждый $i$ класс, посмотрели на численность их внутри данной выборке (разделенной), просуммировали варианты и вычли - получили прирост информации.

- *Индекс Джини*. Мера неоднородности данных в узле. Чем меньше индекс Джини, тем чище узел.
- *GainRation*. Мы берем информационный прирост и делим его на энтропию родительской (неразделенной) выборки:

  $$
    \text{GainRation}(S) = \dfrac{\text{IGain}(S)}{\text{Entropy}(S)}
  $$

Ещё можно ограничить свойства дерева с помощью ограничения высоты дерева (максимальная глубина), ограничения минимального информационного прироста (ветвистость и глубина), ограничения минимального количества объектов в узле.
