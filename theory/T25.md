# Дропаут. Пакетная нормализация

## Дропаут

### Основы дропаута (Dropout)

**Искусственная нейронная сеть (ANN)**:

- Нейронная сеть состоит из множества нейронов, соединённых между собой. Каждый нейрон принимает входные данные, обрабатывает их и выдаёт выходное значение.

#### Проблема переобучения

- Переобучение происходит, когда модель слишком хорошо запоминает обучающие данные, но плохо обобщает на новые данные.
- Это приводит к тому, что модель хорошо работает на обучающей выборке, но плохо на тестовой или новой выборке.

### Про дропаут

- Дропаут – это техника регуляризации, которая помогает предотвратить переобучение.
- Она была предложена Hinton et al. в 2012 году.

#### Как работает дропаут

- **Основная идея дропаута**:
  - Во время обучения на каждом шаге случайным образом "выключается" (устанавливается в ноль) определённый процент нейронов.
  - Это предотвращает модель от слишком сильной зависимости от конкретных нейронов и способствует лучшей обобщающей способности.
- **Математическое описание**:
  - Для каждого нейрона с вероятностью $p$ он "выключается" (устанавливается в ноль).
  - Например, если $p=0.5$, то на каждом шаге половина нейронов в слое будут выключены.

#### Как работает предсказание с дропаутом

1. **Обучение**:
    - Во время обучения дропаут активно применяется, случайным образом выключая часть нейронов на каждом шаге.
    - Это помогает предотвратить переобучение, так как модель не может полагаться на конкретные нейроны и вынуждена учить более устойчивые признаки.
2. **Предсказание (Inference)**:
    - Во время предсказания (инференса) дропаут не применяется, все нейроны используются.
    - Однако, чтобы компенсировать отсутствие дропаута, веса нейронов масштабируются. Это делается для того, чтобы сохранить тот же уровень активаций, что и во время обучения.
3. **Математическое описание**:
    - Если во время обучения каждый нейрон активен с вероятностью $p$, то во время предсказания его активации умножаются на $p$.
    - Например, если $p=0.5$, то активации умножаются на $0.5$ во время предсказания.

#### Почему это работает

- **Усреднение**: Дропаут можно рассматривать как усреднение множества различных нейронных сетей с разными подмножествами нейронов.
- **Регуляризация**: Он действует как метод регуляризации, снижая вероятность переобучения и улучшая способность модели к обобщению.

## Пакетная нормализация

### Проблема обучения глубоких сетей

- В глубоких нейронных сетях, особенно с большим количеством слоёв, обучение может быть сложным из-за нестабильных градиентов и разных масштабов значений на разных слоях.

### Пакетная нормализация (Batch Normalization)

- Пакетная нормализация – это метод, который помогает стабилизировать и ускорить обучение нейронных сетей.
- Она была предложена Sergey Ioffe и Christian Szegedy в 2015 году.

#### Как работает пакетная нормализация

**Основная идея пакетной нормализации**:

- На каждом слое нормализуются входные данные, то есть приводятся к одинаковому масштабу.
- Это делается путём вычитания среднего значения и деления на стандартное отклонение для каждого мини-пакета данных.
**Преобразование данных**:
- Пусть $x$ – входные данные слоя.
- Для каждого мини-пакета данных вычисляются среднее значение ($\mu$) и дисперсия ($\sigma^2$).
  - Нормализованные данные вычисляются по формуле:
    $$
    \hat{x} = \frac{x - \mu}{\sqrt{\sigma^2 + \epsilon}}
    $$
  - Здесь $\mu$ – среднее значение мини-пакета, $\sigma^2$ – дисперсия мини-пакета, и $\epsilon$ – маленькое число для избежания деления на ноль.
**Обратная трансформация**:
- После нормализации данные масштабируются и смещаются с помощью параметров $\gamma$ и $\beta$:

$$
y = \gamma\hat{x} + \beta
$$

- Эти параметры обучаются вместе с остальными параметрами сети.
- $\gamma$ и $\beta$ помогают сети восстанавливать представление данных, если это необходимо.

#### Почему пакетная нормализация работает

**Стабилизация обучения**:

- Нормализация входных данных на каждом слое уменьшает изменения в распределении активаций между слоями.
- Это помогает избежать проблемы "взрывающихся" или "затухающих" градиентов, делая обучение более стабильным.
**Ускорение обучения**:
- Нормализованные данные позволяют использовать большие скорости обучения, что ускоряет процесс обучения.
- Это также уменьшает зависимость от начальной инициализации весов.
**Регуляризация**:
- Пакетная нормализация действует как метод регуляризации, добавляя небольшую шумовую компоненту в данные.
- Это помогает уменьшить вероятность переобучения, улучшая способность модели к обобщению.

#### Преимущества пакетной нормализации

- **Стабилизирует обучение**: Уменьшает колебания в распределении активаций между слоями.
- **Ускоряет обучение**: Позволяет использовать более высокие скорости обучения.
- **Действует как регуляризация**: Добавляет шум в данные, уменьшая вероятность переобучения.
- **Уменьшает зависимость от инициализации весов**: Делает обучение менее чувствительным к начальной инициализации весов.

#### Что за обратная трансформация и зачем она нужна?

**Обратная трансформация**:

- После того как входные данные слоя нормализованы, мы применяем обратную трансформацию с помощью параметров $\gamma$ (масштабирование) и $\beta$ (смещение).
**Зачем она нужна**:
- Нормализация данных помогает стабилизировать обучение, но если бы мы оставили данные в нормализованном виде, сеть могла бы потерять важные признаки, нужные для обучения.
- Параметры $\gamma$ и $\beta$ позволяют сети адаптировать нормализованные данные к более подходящему масштабу и смещению, если это необходимо.
**Когда она применяется**:
- Обратная трансформация применяется сразу после нормализации данных на каждом слое.
- Это происходит на каждом шаге обучения и инференса (предсказания).
